{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Stage1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"-0TlG8LHCzSg","executionInfo":{"status":"ok","timestamp":1634618412536,"user_tz":-330,"elapsed":25915,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["from __future__ import print_function\n","import os, sys, gc, argparse, numpy as np\n","\n","import torch\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.utils.data"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wf4pNg-AFsSx","executionInfo":{"status":"ok","timestamp":1634618525510,"user_tz":-330,"elapsed":112997,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}},"outputId":"9ac268b1-2b15-471a-ab75-2f11c208753d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","print(\"/content/gdrive/MyDrive/Datascience/ARProject/Tryon\")\n","\n","sDrive = \"/content/gdrive/MyDrive/Datascience/ARProject/Tryon\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Datascience/ARProject/Tryon\n"]}]},{"cell_type":"code","metadata":{"id":"dCm4GqbPD9z8","executionInfo":{"status":"ok","timestamp":1634618525511,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["# set the default values to run the training model\n","def get_options():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"dataroot\", type=str, default=\"data\")\n","    parser.add_argument(\"datamode\", default=\"train\")\n","    parser.add_argument(\"stage\", default=\"Stitch\", help='Shape, Stitch, Refine')\n","    parser.add_argument(\"data_list\", default=\"train_pairs.txt\")\n","    parser.add_argument(\"thread\", default=\"0\") # number of workers/thread to use for loading data\n","    parser.add_argument('batch', type=str, default=\"1\")  # batch size\n","    parser.add_argument('results', type=str, default='results/Shape', help='save results')\n","    parser.add_argument(\"epochs\", type=str, default=\"45\")\n","    parser.add_argument(\"input_channel\", type=str, default=\"6\")\n","    parser.add_argument(\"decay_epoch\", type=str, default=\"10\")\n","    parser.add_argument('learn_rate', type=str, default=\"0.0002\", help='initial learning rate for adam')\n","    parser.add_argument(\"critic\", type=str, default=\"10\")  # Number of times after which to update Discriminator.\n","    parser.add_argument(\"display_count\", type=str, default=\"1000\")\n","    parser.add_argument(\"save_model\", type=str, default=\"2\")\n","    # set default values\n","    argv = [\"\", \"Data\", \"train\", \"Shape\", \"train_pairs.txt\", \"0\", \"1\", \"results/\"\n","            , \"20\", \"6\", \"10\", \"0.0002\", \"10\", \"1000\", \"2\"]\n","    opt = parser.parse_args(argv[1:])\n","    print(\"arguments are set for training the model\")\n","    return opt"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hglrpaKsEPep","executionInfo":{"status":"ok","timestamp":1634618527176,"user_tz":-330,"elapsed":1668,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from imgaug import augmenters as iaa\n","from torch.autograd import Variable\n","\n","\n","# Initialize kernel weights to uniform. We are not using BatchNorm in final code.\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1:\n","        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant(m.bias.data, 0.0)\n","\n","\n","# LambdaLR is use for Learning rate scheduling (Not used in main code).\n","class LambdaLR():\n","    def __init__(self, n_epochs, offset, decay_start_epoch):\n","        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n","        self.n_epochs = n_epochs\n","        self.offset = offset\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch):\n","        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n","\n","\n","class commonFunctions:\n","    def name(self):\n","        return 'commonFunctions'\n","\n","    def __init__(self):\n","        super(commonFunctions, self).__init__()\n","\n","    def createDir(self, path):\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","\n","    def display_img(self, img, cmap=None):\n","        fig = plt.figure(figsize=(12, 10))\n","        ax = fig.add_subplot(111)\n","        ax.imshow(img, cmap)\n","\n","\n","class ImgAugTransform:\n","    def __init__(self):\n","        self.aug = iaa.Sequential([\n","            #         iaa.Scale((128, 128)),\n","            #         iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n","            #         iaa.Fliplr(0.5),\n","            #         iaa.Affine(rotate=(-40, 40), mode='symmetric'),\n","            iaa.Affine(rotate=40, mode='symmetric')\n","            #         iaa.Sometimes(0.25,\n","            #                       iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n","            #                                  iaa.CoarseDropout(0.1, size_percent=0.5)])),\n","            #         iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n","        ])\n","\n","    def __call__(self, img):\n","        img = np.array(img)\n","        return self.aug.augment_image(img)\n","\n","\n","class ImgAugTransformStitching:\n","    def __init__(self):\n","        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","        self.aug = iaa.Sequential([\n","#         iaa.Scale((128, 128)),\n","#         iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n","#         iaa.Fliplr(0.5),\n","        iaa.Affine(rotate=40, mode='symmetric'),\n","#             iaa.Affine( rotate = 20 , mode='symmetric')\n","#         iaa.Sometimes(0.25,\n","#                       iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n","#                                  iaa.CoarseDropout(0.1, size_percent=0.5)])),\n","#         iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n","        iaa.Affine(\n","            translate_percent={\"x\":0.2, \"y\": 0.1},\n","#             rotate=(-45, 45),\n","#             shear=(-16, 16),\n","#             order=[0, 1],\n","#             cval=(0, 255),\n","            mode='symmetric'\n","        )\n","    ])\n","    def __call__(self, img, img1, img2):\n","        img = np.array(img)\n","        img1 = np.array(img1)\n","        img2 = np.array(img2)\n","\n","        return self.aug.augment_image(img), self.aug.augment_image(img1), self.aug.augment_image(img2)\n","\n","class ImgAugTransformRefine:\n","    def __init__(self):\n","        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","        self.aug = iaa.Sequential([\n","        iaa.Affine(\n","            translate_percent={\"x\":0.2, \"y\": 0.1},\n","            mode='symmetric'\n","        )\n","    ])\n","    def __call__(self, img, img1, img2):\n","        img = np.array(img)\n","        img1 = np.array(img1)\n","        img2 = np.array(img2)\n","\n","        return self.aug.augment_image(img), self.aug.augment_image(img1), self.aug.augment_image(img2)\n","\n","# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n","# ReplayBuffer was first introduced in the above mentioned paper, It's effect mathematically has been supported in\n","# latest ICLR paper ProbGAN. Replay buffer uses previous data as prior for the Discriminator which it has seen already.\n","# Page 5 of the paper, just over Theory section.\n","# Hence we propose to maintain a subset of discriminators by subsampling the whole sequence of discriminators.\n","\n","class ReplayBuffer():\n","    def __init__(self, max_size=50):\n","        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0, 1) > 0.5:\n","                    i = random.randint(0, self.max_size - 1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRos9lXHEEMV","executionInfo":{"status":"ok","timestamp":1634618528109,"user_tz":-330,"elapsed":936,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import numpy as np\n","import os\n","import os.path as osp\n","import matplotlib.pyplot as plt\n","import json\n","import random\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","from skimage.filters import threshold_otsu\n","from PIL import Image\n","import torchvision.transforms.functional as TF\n","from PIL import ImageDraw\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","class shapeDataSetExtract():\n","    def __init__(self=None, height=0):\n","        super(shapeDataSetExtract, self).__init__()\n","        # base setting\n","        path_ = os.getcwd()\n","        print(path_)\n","        self.root = path_ + '/gdrive/MyDrive/Datascience/ARProject/Tryon/data/'\n","        print(self.root)\n","        self.datamode = 'train'  # train or test or self-define\n","        self.data_list = \"train_pairs.txt\"\n","        self.fine_height = height\n","        self.fine_width = 128\n","        self.radius = 3\n","        self.data_path = osp.join(self.root, self.datamode)\n","\n","        self.transform = transforms.Compose(\n","            (transforms.Scale(self.fine_height), transforms.ToTensor(), transforms.Normalize(0.5, 0.5)))\n","\n","        self.transform_input = transforms.Compose([ImgAugTransform(),\n","                                                   transforms.ToTensor(),\n","                                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","        print('setup shape Dataset')\n","        print(self.data_path)\n","        # load data list\n","        im_names = []\n","        c_names = []\n","        with open(osp.join(self.root, self.data_list), 'r') as f:\n","            for line in f.readlines():\n","                im_name, c_name = line.strip().split()\n","                im_names.append(im_name)\n","                c_names.append(c_name)\n","\n","        self.im_names = im_names\n","        self.c_names = c_names\n","        self.rotate = ImgAugTransform()\n","\n","        #print(self.im_names)\n","        #print(self.c_names)\n","\n","        print('init is done.')\n","\n","    def name(self):\n","        return \"PolyDatasetShape\"\n","\n","    def transformData(self, src, mask, target, cloth, skel, face):\n","        # Resize\n","        resize = transforms.Resize(size=(128, 128))\n","        src = resize(src)  # Source with missing cloth\n","        mask = resize(mask)  # mask of the missing cloth\n","        target = resize(target)  # target/ Ground truth\n","        cloth = resize(cloth)  # Cloth ground truth, how it should look before applying\n","        skel = resize(skel)  # skeleton\n","        face = resize(face) #face\n","\n","\n","        if random.random() > 0.5:\n","            src = self.rotate(src)\n","            mask = self.rotate(mask)\n","            target = self.rotate(target)\n","            cloth = self.rotate(cloth)\n","            skel = self.rotate(skel)\n","\n","            # Transform to tensor\n","\n","        src = TF.to_tensor(src)\n","        mask = TF.to_tensor(mask)\n","        target = TF.to_tensor(target)\n","        cloth = TF.to_tensor(cloth)\n","        skel = TF.to_tensor(skel)\n","\n","        src = TF.normalize(src, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        mask = TF.normalize(mask, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        target = TF.normalize(target, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        cloth = TF.normalize(cloth, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        skel = TF.normalize(skel, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        return src, mask, target, cloth, skel, face\n","\n","    def get_binary_from_img(self, image_name):\n","        loader2 = transforms.Compose([transforms.Resize((256, 192)), transforms.ToTensor()])\n","        \"\"\"load image, returns cuda tensor\"\"\"\n","        image = Image.fromarray(np.uint8(image_name))\n","        image = loader2(image).float()\n","        better_contrast = image.permute(1, 2, 0).detach().cpu().numpy()\n","        better_contrast[better_contrast > 1] = 1\n","        #     print(lol.shape)\n","\n","        thresh = threshold_otsu(better_contrast)\n","        binary = better_contrast > thresh\n","        return binary  # assumes that you're using GPU\n","\n","    def get_binary(self, image_name):\n","        loader2 = transforms.Compose([transforms.Resize((256, 192)), transforms.ToTensor()])\n","        \"\"\"load image, returns cuda tensor\"\"\"\n","        image = Image.open(image_name)\n","        image = loader2(image).float()\n","        better_contrast = image.permute(1, 2, 0).detach().cpu().numpy()\n","        better_contrast[better_contrast > 1] = 1\n","        #     print(lol.shape)\n","\n","        thresh = threshold_otsu(better_contrast)\n","        binary = better_contrast > thresh\n","        return binary  # assumes that you're using GPU\n","\n","    def __getitem__(self, index):\n","        c_name = self.c_names[index]\n","        im_name = self.im_names[index]\n","\n","        # person image\n","        im = plt.imread(osp.join(self.data_path, 'image', im_name))\n","        cm = plt.imread(osp.join(self.data_path, 'cloth', c_name))\n","        #         im = self.transform(im) # [-1,1]\n","\n","        # load parsing image\n","\n","        parse_name = im_name.replace('.jpg', '.png')\n","        im_parse = Image.open(osp.join(self.data_path, 'image-parse', parse_name))\n","        parse_array = np.array(im_parse)\n","        #         parse_shape = (parse_array > 0).astype(np.float32)\n","        parse_head = (parse_array == 1).astype(np.float32) + \\\n","                     (parse_array == 2).astype(np.float32) + \\\n","                     (parse_array == 4).astype(np.float32) + \\\n","                     (parse_array == 13).astype(np.float32)\n","\n","        parse_cloth = (parse_array == 5).astype(np.float32) + \\\n","                      (parse_array == 6).astype(np.float32) + \\\n","                      (parse_array == 7).astype(np.float32) + \\\n","                      (parse_array == 9).astype(np.float32) + \\\n","                      (parse_array == 15).astype(np.float32) + (parse_array == 3).astype(np.float32) + (\n","                                  parse_array == 14).astype(np.float32)\n","\n","        pcm = self.get_binary_from_img(parse_cloth)\n","        phead = self.get_binary_from_img(parse_head)  # [0,1]\n","        im_h = im * phead - (1 - phead)\n","\n","        #         im_c = im * pcm + (1 - pcm) # [-1,1], fill 1 for other parts\n","        ##### Create Skeleton #################\n","\n","        pose_name = im_name.replace('.jpg', '_keypoints.json')\n","        \n","        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n","            pose_label = json.load(f)\n","            pose_data = pose_label['people'][0]['pose_keypoints']\n","            pose_data = np.array(pose_data)\n","            pose_data = pose_data.reshape((-1, 3))\n","\n","        point_num = pose_data.shape[0]\n","        r = 7  # self.radius\n","        #         pdb.set_trace()\n","        coop = {}\n","        coop2 = {}\n","        ai = 0\n","        for lol, i in enumerate(\n","                [1, 2, 3, 4, 5, 6, 7, 8, 11]):  # leaving out head and legs joints, heap and hands are kept\n","            pointx = pose_data[i, 0]\n","            pointy = pose_data[i, 1]\n","            if pointx > 1 and pointy > 1:\n","                coop[ai] = (pointx, pointy)\n","                coop2[ai] = (pointx, pointy)\n","                ai = ai + 1\n","            else:\n","                coop2[ai] = (pointx, pointy)\n","                ai = ai + 1\n","\n","        # creating skeleton\n","        bone_list = [[x[0], x[1]] for key, x in coop2.items()]\n","        #         bone_list = bone_list.numpy()\n","        bone_list = np.array(bone_list) - 1\n","        itemindex = np.where(bone_list == -1)\n","        if len(itemindex[0]) == 0:\n","            it = 100\n","        else:\n","            it = np.unique(itemindex[0])\n","\n","        one_map = Image.new('RGB', (192, 256))\n","        draw = ImageDraw.Draw(one_map)\n","        if np.logical_not(np.isin(it, 0)).all() and np.logical_not(np.isin(it, 1)).all():\n","            draw.line((bone_list[0][0], bone_list[0][1], bone_list[1][0], bone_list[1][1]), fill='red', width=14)\n","        if np.logical_not(np.isin(it, 1)).all() and np.logical_not(np.isin(it, 2)).all():\n","            draw.line((bone_list[1][0], bone_list[1][1], bone_list[2][0], bone_list[2][1]), fill='blue', width=14)\n","        if np.logical_not(np.isin(it, 3)).all() and np.logical_not(np.isin(it, 2)).all():\n","            draw.line((bone_list[2][0], bone_list[2][1], bone_list[3][0], bone_list[3][1]), fill='white', width=14)\n","        if np.logical_not(np.isin(it, 0)).all() and np.logical_not(np.isin(it, 4)).all():\n","            draw.line((bone_list[0][0], bone_list[0][1], bone_list[4][0], bone_list[4][1]), fill='orange', width=14)\n","        if np.logical_not(np.isin(it, 4)).all() and np.logical_not(np.isin(it, 5)).all():\n","            draw.line((bone_list[4][0], bone_list[4][1], bone_list[5][0], bone_list[5][1]), fill='orchid', width=14)\n","        if np.logical_not(np.isin(it, 6)).all() and np.logical_not(np.isin(it, 5)).all():\n","            draw.line((bone_list[5][0], bone_list[5][1], bone_list[6][0], bone_list[6][1]), fill='yellow', width=14)\n","\n","        if np.logical_not(np.isin(it, 0)).all() and np.logical_not(np.isin(it, 1)).all():\n","            draw.line((bone_list[1][0], bone_list[1][1], bone_list[7][0], bone_list[7][1]), fill='gold', width=14)\n","        if np.logical_not(np.isin(it, 4)).all() and np.logical_not(np.isin(it, 8)).all():\n","            draw.line((bone_list[4][0], bone_list[4][1], bone_list[8][0], bone_list[8][1]), fill='pink', width=14)\n","        if np.logical_not(np.isin(it, 7)).all() and np.logical_not(np.isin(it, 8)).all():\n","            draw.line((bone_list[7][0], bone_list[7][1], bone_list[8][0], bone_list[8][1]), fill='brown', width=14)\n","\n","        ###########################################\n","\n","        source = im * pcm\n","        source[source == 0] = 255\n","        mask = plt.imread(osp.join(self.data_path, 'nested_unet_msk', im_name))\n","\n","        lol = self.get_binary(osp.join(self.data_path, 'nested_unet_msk', im_name))\n","        lol2 = source * (1 - lol)\n","        lol2[lol2 == 0] = 255\n","\n","        lol3 = source * (lol)\n","        lol3[lol3 == 0] = 255\n","\n","        input = Image.fromarray(np.uint8(lol2))\n","        mask = Image.fromarray(np.uint8(mask))\n","        style = Image.fromarray(np.uint8(lol3))\n","        target = Image.fromarray(np.uint8(source))\n","        cloth = Image.fromarray(np.uint8(cm))\n","        face = Image.fromarray(np.uint8(im_h))\n","\n","        style_ = self.transform(style)\n","        cloth = self.transform(cloth)\n","        face = self.transform(face)\n","        \n","        resize = transforms.Resize(size=(128, 128))\n","        cloth = resize(cloth)  # Cloth ground truth, how it should look before applying\n","        style_ = resize(style_)\n","        \n","        source, mask, target, targ, skel, face = self.transformData(input, mask, target, style, one_map, face)\n","        del lol3, lol2, pcm, im, parse_cloth, im_parse, lol\n","        return source, mask, style_, target, targ, skel, cloth, face  # , skel\n","\n","    def __len__(self):\n","        return len(self.im_names)\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOAVCMA-EaRx","executionInfo":{"status":"ok","timestamp":1634618529107,"user_tz":-330,"elapsed":1004,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","# ResNet module to process the incoming filters. We are using Instance Norm replacing traditional BatchNorm.\n","# BatchNorm doesn't plays any significant role, since our batch is very small, another thing we observed is\n","# that the feature maps don't face covariate shift in ResNet block as the dataset are very close to each other.\n","# removing Norm from ResNet block doesn't affects the model result.\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ResidualBlock, self).__init__()\n","\n","        conv_block = [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, in_features, 3),\n","                      nn.InstanceNorm2d(in_features),\n","                      nn.ReLU(inplace=True),\n","                      nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, out_features, 3),\n","                      nn.InstanceNorm2d(out_features)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ConvBlock, self).__init__()\n","\n","        conv_block = [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, in_features, 3),\n","                      nn.InstanceNorm2d(in_features),\n","                      nn.ReLU(inplace=True),\n","                      nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, out_features, 3),\n","                      nn.InstanceNorm2d(out_features)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","# Over the cause of GAN history we did infer that if we replace unknown region with Noise, then GANs can effectively\n","# generate the missing regions (effectively implies to generate something).\n","# We didn't test with different Noise, and their affects in detail.\n","class NoiseInjection(nn.Module):\n","    def __init__(self, channel):\n","        super().__init__()\n","\n","        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n","\n","    def forward(self, image, mask):\n","        #         pdb.set_trace()\n","        noise = torch.randn(1, 1, image.shape[2], image.shape[3]).cuda()\n","        mask = mask[:, :1, :, :].repeat(1, image.shape[1], 1, 1)\n","        return image + self.weight * noise * mask\n","\n","\n","def swish(x):\n","    return x * F.sigmoid(x)\n","\n","\n","def get_mean_var(c):\n","    n_batch, n_ch, h, w = c.size()\n","\n","    c_view = c.view(n_batch, n_ch, h * w)\n","    c_mean = c_view.mean(2)\n","\n","    c_mean = c_mean.view(n_batch, n_ch, 1, 1).expand_as(c)\n","    c_var = c_view.var(2)\n","    c_var = c_var.view(n_batch, n_ch, 1, 1).expand_as(c)\n","    # c_var = c_var * (h * w - 1) / float(h * w)  # unbiased variance\n","\n","    return c_mean, c_var\n","\n","\n","# model_ds downsamples the feature maps, we use stride = 2 to downsample feature maps instead of\n","# max pooling layer which is not learnable.\n","class model_ds(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(model_ds, self).__init__()\n","\n","        conv_block = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n","                      nn.InstanceNorm2d(out_features),\n","                      nn.ReLU(inplace=True)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","# model_up Upsamples the feature maps again with a layer which is learnable, we didn't use any other method since\n","# nn.Upsample has no learnable weights, the other layer that we could have tried is sub-pixel which also learns to\n","# upsample / downsmaple.\n","class model_up(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(model_up, self).__init__()\n","\n","        conv_block = [nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n","                      nn.InstanceNorm2d(out_features),\n","                      nn.ReLU(inplace=True)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","class transform_layer(nn.Module):\n","\n","    def __init__(self, input_nc, in_features, out_features):\n","        super(transform_layer, self).__init__()\n","        self.channels = in_features\n","\n","        self.convblock = ConvBlock(in_features + in_features, out_features)\n","        self.up_conv = nn.Conv2d(in_features * 2, in_features, 3, 1, 1)\n","        self.down_conv = nn.Sequential(\n","            nn.Conv2d(64, in_features // 4, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_features // 4, in_features // 2, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_features // 2, in_features, 1, 1),\n","            nn.ReLU()\n","        )\n","        self.noise = NoiseInjection(in_features)\n","\n","        self.convblock_ = ConvBlock(in_features + 64, out_features)\n","\n","        self.vgg_block = nn.Sequential(\n","            nn.Conv2d(input_nc, 16, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 1, 1),\n","            nn.ReLU()\n","        )\n","\n","        self.vgg_block = nn.Sequential(\n","            nn.Conv2d(input_nc, 16, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 1, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x, mask=None, style=None, mode='D'):\n","        #         pdb.set_trace()\n","        if mode == 'C':\n","            style = F.upsample(style, size=(x.shape[2], x.shape[2]), mode='bilinear')\n","\n","            style = self.vgg_block(style)\n","            concat = torch.cat([x, style], 1)\n","\n","            out = (self.convblock_(concat))\n","            return out, style\n","        else:\n","            mask = F.upsample(mask, size=(x.shape[2], x.shape[2]), mode='bilinear')\n","            x = self.noise(x, mask)\n","            #             style = F.upsample(style, size=(x.shape[2],x.shape[2]), mode='bilinear')\n","\n","            style = self.down_conv(style)\n","            concat = torch.cat([x, style], 1)\n","\n","            out = (self.convblock(concat) + style)\n","            return out\n","\n","\n","class transform_up_layer(nn.Module):\n","    def __init__(self, in_features, out_features, diff=False):\n","        super(transform_up_layer, self).__init__()\n","        self.channels = in_features\n","\n","        if diff == True:\n","            self.convblock = ConvBlock(in_features * 2 + in_features, out_features)\n","        else:\n","            self.convblock = ConvBlock(in_features * 2, out_features)\n","        self.up_conv = nn.Sequential(\n","            nn.Conv2d(in_features * 2, in_features, 3, 1, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x, y, mode=\"down\"):\n","\n","        y = self.up_conv(y)\n","        concat = torch.cat([x, y], 1)\n","\n","        out = self.convblock(concat)\n","\n","        #         out = self.adain(out,style)\n","\n","        return out\n","\n","\n","class GeneratorCoarse(nn.Module):\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=1):\n","        super(GeneratorCoarse, self).__init__()\n","        in_features = 64\n","\n","        self.model_input_cloth = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_nc + 1, in_features, 7),\n","            nn.InstanceNorm2d(in_features),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.block128 = nn.Sequential(\n","            ResidualBlock(in_features, in_features)\n","        )\n","        self.block128_transform = transform_layer(input_nc, in_features, in_features)\n","\n","        self.block64 = nn.Sequential(\n","            model_ds(in_features, in_features * 2),\n","            ResidualBlock(in_features * 2, in_features * 2)\n","        )\n","        self.block64_transform = transform_layer(input_nc, in_features * 2, in_features * 2)\n","\n","        self.block32 = nn.Sequential(\n","            model_ds(in_features * 2, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block32_transform = transform_layer(input_nc, in_features * 4, in_features * 4)\n","\n","        self.block16 = nn.Sequential(\n","            model_ds(in_features * 4, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block16_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","        self.block8 = nn.Sequential(\n","            model_ds(in_features * 8, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block8_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","        self.block4 = nn.Sequential(\n","            model_ds(in_features * 8, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block4_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","\n","        self.block4_up = nn.Sequential(\n","            nn.Conv2d(in_features * 8, in_features * 4, 3, 1, 1),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block4_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block8_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block8_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block16_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block16_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block32_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block32_up_transform = transform_up_layer(in_features * 2, in_features * 4, True)\n","\n","        self.block64_up = nn.Sequential(\n","            model_up(in_features * 4, in_features * 2),\n","            ResidualBlock(in_features * 2, in_features * 2)\n","        )\n","        self.block64_up_transform = transform_up_layer(in_features, in_features * 2, True)\n","\n","        self.block128_up = nn.Sequential(\n","            model_up(in_features * 2, in_features),\n","            ResidualBlock(in_features, in_features)\n","        )\n","\n","        self.block128_up_transform = transform_up_layer(in_features // 2, in_features, True)\n","\n","        self.model_output = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_features, output_nc, 7),\n","            nn.Tanh()\n","        )\n","    def _conv_layer_set(self, in_c, out_c):\n","        model_input = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_c, out_c, 7, padding=0),\n","            nn.InstanceNorm2d(out_c),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    # forward function\n","    def forward(self, src, *input):\n","        in_features = 64\n","        conds = []\n","        for cond in input:\n","            conds.append(cond)\n","        conds.append(src)\n","\n","        style = torch.cat(conds, 1)\n","        y = torch.cat([torch.randn(1, 1, src.shape[2], src.shape[3]), style], 1)\n","\n","        y = self.model_input_cloth(y)\n","\n","        y128 = self.block128(y)\n","        y128, s_128 = self.block128_transform(x=y128, style=style, mode=\"C\")\n","\n","        y64 = self.block64(y128)\n","        y64, s_64 = self.block64_transform(x=y64, style=style, mode=\"C\")\n","\n","        y32 = self.block32(y64)\n","        y32, s_32 = self.block32_transform(x=y32, style=style, mode=\"C\")\n","\n","        y16 = self.block16(y32)\n","        y16, s_16 = self.block16_transform(x=y16, style=style, mode=\"C\")\n","\n","        y8 = self.block8(y16)\n","        y8, s_8 = self.block8_transform(x=y8, style=style, mode=\"C\")\n","\n","        y4 = self.block4(y8)\n","        y4, s_4 = self.block4_transform(x=y4, style=style, mode=\"C\")\n","\n","        ############## Decoder #######################\n","\n","        y4u = self.block4_up(y4)\n","        y4u = self.block4_up_transform(y4u, y4)\n","\n","        y8u = self.block8_up(y4u)\n","        y8u = self.block8_up_transform(y8u, y8)\n","\n","        y16u = self.block16_up(y8u)\n","        y16u = self.block16_up_transform(y16u, y16)\n","\n","        y32u = self.block32_up(y16u)\n","\n","        y64u = self.block64_up(y32u)\n","\n","        y128u = self.block128_up(y64u)\n","\n","        out = self.model_output(y128u)\n","\n","        return out, s_128, s_64, s_32, s_16, s_8, s_4\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n","\n","        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n","        self.bn2 = nn.InstanceNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n","        self.bn3 = nn.InstanceNorm2d(128)\n","        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n","        self.bn4 = nn.InstanceNorm2d(128)\n","        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n","        self.bn5 = nn.InstanceNorm2d(256)\n","        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n","        self.bn6 = nn.InstanceNorm2d(256)\n","        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n","        self.bn7 = nn.InstanceNorm2d(512)\n","        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n","        self.bn8 = nn.InstanceNorm2d(512)\n","\n","        # Replaced original paper FC layers with FCN\n","        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        x = swish(self.conv1(x))\n","\n","        x = swish(self.bn2(self.conv2(x)))\n","        x = swish(self.bn3(self.conv3(x)))\n","        x = swish(self.bn4(self.conv4(x)))\n","        x = swish(self.bn5(self.conv5(x)))\n","        x = swish(self.bn6(self.conv6(x)))\n","        x = swish(self.bn7(self.conv7(x)))\n","        x = swish(self.bn8(self.conv8(x)))\n","\n","        x = self.conv9(x)\n","        return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLMRpP1WEf4t","executionInfo":{"status":"ok","timestamp":1634618529442,"user_tz":-330,"elapsed":337,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["def trainShapeModel(opt, netG, netD):\n","    print(\"training the model for different model types: %s\" % (opt.stage))\n","    dataset = shapeDataSetExtract(128)\n","    train_loader = DataLoader(dataset,\n","                              batch_size=int(opt.batch),\n","                              shuffle=False,\n","                              num_workers=int(opt.thread),\n","                              drop_last=True, pin_memory=True)\n","\n","    epoch = 0\n","    n_epochs = int(opt.epochs)\n","    decay_epoch = int(opt.decay_epoch)\n","    batchSize = int(opt.batch)\n","    size = 128\n","    input_nc = int(opt.input_channel)\n","    output_nc = 3\n","    lr = float(opt.learn_rate)\n","    nRow = 3\n","\n","    criterion_GAN = torch.nn.MSELoss()\n","    criterion_identity = torch.nn.L1Loss()\n","\n","    optimizer_G = torch.optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n","    optimizer_D = torch.optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n","\n","    lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G,\n","                                                       lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D,\n","                                                       lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","\n","    # Inputs & targets memory allocation\n","    Tensor = torch.FloatTensor\n","    input_A = Tensor(batchSize,input_nc,size,size)\n","\n","    target_real = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n","    target_fake = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n","\n","    fake_buffer = ReplayBuffer()\n","    print(n_epochs)\n","\n","    for epoch in range(0, n_epochs):\n","\n","        gc.collect()\n","        print(epoch)\n","        Source = iter(train_loader)\n","        avg_loss_g = 0\n","        avg_loss_d = 0\n","        for i in range(0, len(train_loader)):\n","            netG.train()\n","            target_real = Variable(torch.ones(1, 1), requires_grad=False)\n","            target_fake = Variable(torch.zeros(1, 1), requires_grad=False)\n","            optimizer_G.zero_grad()\n","\n","            src, mask, style_img, target, gt_cloth, skel, cloth, face = Source.next()\n","            src, mask, style_img, target, gt_cloth, skel, cloth, face = Variable(src), Variable(mask), Variable(style_img), \\\n","                                                                  Variable(target), Variable(gt_cloth), Variable(skel), Variable(cloth), Variable(face)\n","\n","\n","            #inverse identity\n","            gen_targ, _, _, _, _, _, _ = netG(skel, cloth)  # src,conditions\n","\n","            pred_fake = netD(gen_targ)\n","            loss_GAN = 10 * criterion_GAN(pred_fake, target_real) + 10 * criterion_identity(gen_targ, gt_cloth)\n","\n","            loss_G = loss_GAN\n","            loss_G.backward()\n","\n","            optimizer_G.step()\n","\n","            optimizer_D.zero_grad()\n","\n","            pred_real = netD(gt_cloth)\n","\n","            loss_D_real = criterion_GAN(pred_real, target_real)\n","\n","            # Fake loss\n","            gen_targ = fake_buffer.push_and_pop(gen_targ)\n","            pred_fake = netD(gen_targ.detach())\n","            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n","\n","            # Total loss\n","            loss_D = (loss_D_real + loss_D_fake) * 0.5\n","            loss_D.backward()\n","            if (i + 1) % int(opt.critic) == 0:\n","                optimizer_D.step()\n","\n","            avg_loss_g = (avg_loss_g + loss_G) / (i + 1)\n","            avg_loss_d = (avg_loss_d + loss_D) / (i + 1)\n","\n","            path_ = os.getcwd()\n","            root = path_ + '/gdrive/MyDrive/Datascience/ARProject/Tryon'\n","\n","            if (i + 1) % 50 == 0:\n","                print(\"Epoch: (%3d) (%5d/%5d) Loss: (%0.0003f) (%0.0003f)\" % (\n","                epoch, i + 1, len(train_loader), avg_loss_g * 1000, avg_loss_d * 1000))\n","\n","            #if (i + 1) % int(opt.display_count) == 0:\n","                pic = (torch.cat([cloth, skel, gt_cloth ], dim=0).data + 1) / 2.0\n","                pic1 = (torch.cat([src, mask, style_img, target, gt_cloth, skel, cloth, face], dim=0).data + 1) / 2.0\n","                \n","                \n","                save_dir = \"{}/{}{}\".format(root, opt.results, opt.stage)\n","                \n","                #print(save_dir)\n","                if not os.path.exists(save_dir):\n","                  os.makedirs(save_dir)\n","                #os.mkdir(save_dir)\n","                save_image(pic, '%s/Epoch_(%d)_(%dof%d).jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","                save_image(pic1, '%s/Epoch_(%d)_(%dof%d)pic1.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","                #save_image(style_img, '%s/Epoch_(%d)_(%dof%d)style_img.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","\n","                #save_image(gen_targ, '%s/Epoch_(%d)_(%dof%d)gen_targ.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","\n","                #save_image(skel, '%s/Epoch_(%d)_(%dof%d)skel.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","                #save_image(src, '%s/Epoch_(%d)_(%dof%d)target.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","                #save_image(gt_cloth, '%s/Epoch_(%d)_(%dof%d)gt_cloth.jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","            if (epoch + 1) % int(opt.save_model) == 0:\n","                save_dir = \"{}/{}{}\".format(root, opt.results, opt.stage)\n","                torch.save(netG.state_dict(), '{}/Gan_{}.pth'.format(save_dir, epoch))\n","                # Update learning rates\n","\n","            lr_scheduler_G.step()\n","            lr_scheduler_D.step()\n","            #if(i ==2):\n","              #break;\n","        #if(epoch ==2):\n","          #break;"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6RL2J0lEo1L","executionInfo":{"status":"ok","timestamp":1634618529443,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["# define main function to start executing the training model\n","def main():\n","    trn_options = get_options()\n","    print(trn_options)\n","    print(\"Model training started\")\n","\n","    if not os.path.exists(trn_options.results):\n","        os.makedirs(trn_options.results)\n","    \n","    if trn_options.stage == \"Stitch\":\n","        netG = GeneratorCoarse(9, 3)\n","    else:\n","        netG = GeneratorCoarse(int(trn_options.input_channel), 3)\n","\n","    netD = Discriminator()\n","\n","    #intialize the weight for the model\n","    netG.apply(weights_init_normal)\n","    netD.apply(weights_init_normal)\n","\n","    if trn_options.stage == \"Shape\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainShapeModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    elif trn_options.stage == \"Stitch\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainStitchModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    elif trn_options.stage == \"Refine\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainRefineModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    else:\n","        print(\"Please mention the Stage from [Shape, Stitch, Refine]\")\n","\n","\n","    print('Finished training ')\n","\n","    sys.exit(\"Please mention the next Stage from [Shape, Stitch, Refine]\")\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIz6gOmGEwNH","colab":{"base_uri":"https://localhost:8080/","height":581},"executionInfo":{"status":"error","timestamp":1634618983016,"user_tz":-330,"elapsed":453577,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}},"outputId":"79f48553-22ad-44d6-d251-69a44e832a4c"},"source":["if __name__ == \"__main__\":\n","    main()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["arguments are set for training the model\n","Namespace(batch='1', critic='10', data_list='train_pairs.txt', datamode='train', dataroot='Data', decay_epoch='10', display_count='1000', epochs='20', input_channel='6', learn_rate='0.0002', results='results/', save_model='2', stage='Shape', thread='0')\n","Model training started\n","Training started for Shape\n","training the model for different model types: Shape\n","/content\n","/content/gdrive/MyDrive/Datascience/ARProject/Tryon/data/\n","setup shape Dataset\n","/content/gdrive/MyDrive/Datascience/ARProject/Tryon/data/train\n","init is done.\n","20\n","0\n","Epoch: (  0) (   50/ 1517) Loss: (398.790) (5.194)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-5764c7a0ba78>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrn_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training started for %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrainShapeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed for %s \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-d0d4c5a558f4>\u001b[0m in \u001b[0;36mtrainShapeModel\u001b[0;34m(opt, netG, netD)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_cloth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_cloth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                                   \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_cloth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-460d2b21a118>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mparse_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mim_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image-parse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mparse_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m#         parse_shape = (parse_array > 0).astype(np.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}
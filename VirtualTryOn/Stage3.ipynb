{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Stage3.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"-0TlG8LHCzSg","executionInfo":{"status":"ok","timestamp":1634620877378,"user_tz":-330,"elapsed":25479,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["from __future__ import print_function\n","import os, sys, gc, argparse, numpy as np\n","\n","import torch\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.utils.data"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wf4pNg-AFsSx","executionInfo":{"status":"ok","timestamp":1634620909630,"user_tz":-330,"elapsed":32265,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}},"outputId":"33579cb4-19fc-4729-875c-fe0d5bfeccc0"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","print(\"/content/gdrive/MyDrive/Datascience/ARProject/Tryon\")\n","\n","sDrive = \"/content/gdrive/MyDrive/Datascience/ARProject/Tryon\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Datascience/ARProject/Tryon\n"]}]},{"cell_type":"code","metadata":{"id":"dCm4GqbPD9z8","executionInfo":{"status":"ok","timestamp":1634620909630,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["# set the default values to run the training model\n","def get_options():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"dataroot\", type=str, default=\"data\")\n","    parser.add_argument(\"datamode\", default=\"train\")\n","    parser.add_argument(\"stage\", default=\"Stitch\", help='Shape, Stitch, Refine')\n","    parser.add_argument(\"data_list\", default=\"train_pairs.txt\")\n","    parser.add_argument(\"thread\", default=\"0\") # number of workers/thread to use for loading data\n","    parser.add_argument('batch', type=str, default=\"1\")  # batch size\n","    parser.add_argument('results', type=str, default='results/Shape', help='save results')\n","    parser.add_argument(\"epochs\", type=str, default=\"45\")\n","    parser.add_argument(\"input_channel\", type=str, default=\"6\")\n","    parser.add_argument(\"decay_epoch\", type=str, default=\"10\")\n","    parser.add_argument('learn_rate', type=str, default=\"0.0002\", help='initial learning rate for adam')\n","    parser.add_argument(\"critic\", type=str, default=\"10\")  # Number of times after which to update Discriminator.\n","    parser.add_argument(\"display_count\", type=str, default=\"1000\")\n","    parser.add_argument(\"save_model\", type=str, default=\"2\")\n","    # set default values\n","    argv = [\"\", \"Data\", \"train\", \"Refine\", \"train_pairs.txt\", \"0\", \"1\", \"results/\"\n","            , \"21\", \"6\", \"10\", \"0.0002\", \"10\", \"500\", \"2\"]\n","    opt = parser.parse_args(argv[1:])\n","    print(\"arguments are set for training the model\")\n","    return opt"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hglrpaKsEPep","executionInfo":{"status":"ok","timestamp":1634620911476,"user_tz":-330,"elapsed":1851,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from imgaug import augmenters as iaa\n","from torch.autograd import Variable\n","\n","\n","# Initialize kernel weights to uniform. We are not using BatchNorm in final code.\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1:\n","        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm2d') != -1:\n","        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant(m.bias.data, 0.0)\n","\n","\n","# LambdaLR is use for Learning rate scheduling (Not used in main code).\n","class LambdaLR():\n","    def __init__(self, n_epochs, offset, decay_start_epoch):\n","        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n","        self.n_epochs = n_epochs\n","        self.offset = offset\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch):\n","        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n","\n","\n","class commonFunctions:\n","    def name(self):\n","        return 'commonFunctions'\n","\n","    def __init__(self):\n","        super(commonFunctions, self).__init__()\n","\n","    def createDir(self, path):\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","\n","    def display_img(self, img, cmap=None):\n","        fig = plt.figure(figsize=(12, 10))\n","        ax = fig.add_subplot(111)\n","        ax.imshow(img, cmap)\n","\n","\n","class ImgAugTransform:\n","    def __init__(self):\n","        self.aug = iaa.Sequential([\n","            #         iaa.Scale((128, 128)),\n","            #         iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n","            #         iaa.Fliplr(0.5),\n","            #         iaa.Affine(rotate=(-40, 40), mode='symmetric'),\n","            iaa.Affine(rotate=40, mode='symmetric')\n","            #         iaa.Sometimes(0.25,\n","            #                       iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n","            #                                  iaa.CoarseDropout(0.1, size_percent=0.5)])),\n","            #         iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n","        ])\n","\n","    def __call__(self, img):\n","        img = np.array(img)\n","        return self.aug.augment_image(img)\n","\n","\n","class ImgAugTransformStitching:\n","    def __init__(self):\n","        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","        self.aug = iaa.Sequential([\n","#         iaa.Scale((128, 128)),\n","#         iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n","#         iaa.Fliplr(0.5),\n","        iaa.Affine(rotate=40, mode='symmetric'),\n","#             iaa.Affine( rotate = 20 , mode='symmetric')\n","#         iaa.Sometimes(0.25,\n","#                       iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n","#                                  iaa.CoarseDropout(0.1, size_percent=0.5)])),\n","#         iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n","        iaa.Affine(\n","            translate_percent={\"x\":0.2, \"y\": 0.1},\n","#             rotate=(-45, 45),\n","#             shear=(-16, 16),\n","#             order=[0, 1],\n","#             cval=(0, 255),\n","            mode='symmetric'\n","        )\n","    ])\n","    def __call__(self, img, img1, img2):\n","        img = np.array(img)\n","        img1 = np.array(img1)\n","        img2 = np.array(img2)\n","\n","        return self.aug.augment_image(img), self.aug.augment_image(img1), self.aug.augment_image(img2)\n","\n","class ImgAugTransformRefine:\n","    def __init__(self):\n","        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","        self.aug = iaa.Sequential([\n","        iaa.Affine(\n","            translate_percent={\"x\":0.2, \"y\": 0.1},\n","            mode='symmetric'\n","        )\n","    ])\n","    def __call__(self, img, img1, img2):\n","        img = np.array(img)\n","        img1 = np.array(img1)\n","        img2 = np.array(img2)\n","\n","        return self.aug.augment_image(img), self.aug.augment_image(img1), self.aug.augment_image(img2)\n","\n","# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n","# ReplayBuffer was first introduced in the above mentioned paper, It's effect mathematically has been supported in\n","# latest ICLR paper ProbGAN. Replay buffer uses previous data as prior for the Discriminator which it has seen already.\n","# Page 5 of the paper, just over Theory section.\n","# Hence we propose to maintain a subset of discriminators by subsampling the whole sequence of discriminators.\n","\n","class ReplayBuffer():\n","    def __init__(self, max_size=50):\n","        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0, 1) > 0.5:\n","                    i = random.randint(0, self.max_size - 1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRos9lXHEEMV","executionInfo":{"status":"ok","timestamp":1634620912017,"user_tz":-330,"elapsed":547,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import numpy as np\n","import os\n","import os.path as osp\n","import matplotlib.pyplot as plt\n","import json\n","import random\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","from skimage.filters import threshold_otsu\n","from PIL import Image\n","import torchvision.transforms.functional as TF\n","from PIL import ImageDraw\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","class refineDataSetExtract():\n","    def __init__(self=None, height=0):\n","        super(refineDataSetExtract, self).__init__()\n","        # base setting\n","        path_ = os.getcwd()\n","        self.root = path_ + '/gdrive/MyDrive/Datascience/ARProject/Tryon/data/'\n","        self.datamode = 'train'  # train or test or self-define\n","        self.data_list = \"train_pairs.txt\"\n","        self.fine_height = height\n","        self.fine_width = 128\n","        self.radius = 3\n","        self.data_path = osp.join(self.root, self.datamode)\n","        self.transform = transforms.Compose(\n","            (transforms.Scale(self.fine_height), transforms.ToTensor(), transforms.Normalize(0.5, 0.5)))\n","\n","        self.transform_input = transforms.Compose(\n","            [ImgAugTransform(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","        # load data list\n","        im_names = []\n","        c_names = []\n","        with open(osp.join(self.root, self.data_list), 'r') as f:\n","            for line in f.readlines():\n","                im_name, c_name = line.strip().split()\n","                im_names.append(im_name)\n","                c_names.append(c_name)\n","\n","        self.im_names = im_names\n","        self.c_names = c_names\n","        self.rotate = ImgAugTransformRefine()\n","\n","    def name(self):\n","        return \"refineDataSetExtract\"\n","\n","    def transformData(self, src, mask, target, cloth, wrap, diff, head):\n","        # Resize\n","        resize = transforms.Resize(size=(128, 128))\n","        src = resize(src)  # Source with missing cloth\n","        mask = resize(mask)  # mask of the missing cloth\n","        target = resize(target)  # target/ Ground truth\n","        cloth = resize(cloth)  # Cloth ground truth, how it should look before applying\n","        wrap = resize(wrap)  # skeleton\n","        diff = resize(diff)\n","        head = resize(head)\n","\n","        src = TF.to_tensor(src)\n","        mask = TF.to_tensor(mask)\n","        target = TF.to_tensor(target)\n","        cloth = TF.to_tensor(cloth)\n","        wrap = TF.to_tensor(wrap)\n","        diff = TF.to_tensor(diff)\n","         #head = TF.to_tensor(head)\n","\n","        src = TF.normalize(src, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        mask = TF.normalize(mask, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        target = TF.normalize(target, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        cloth = TF.normalize(cloth, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        wrap = TF.normalize(wrap, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        diff = TF.normalize(diff, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        return src, mask, target, cloth, wrap, diff, head\n","\n","    def get_binary_from_img(self, image_name):\n","        loader2 = transforms.Compose([transforms.Resize((256, 192)), transforms.ToTensor()])\n","        \"\"\"load image, returns cuda tensor\"\"\"\n","        image = Image.fromarray(np.uint8(image_name))\n","        image = loader2(image).float()\n","        better_contrast = image.permute(1, 2, 0).detach().cpu().numpy()\n","        better_contrast[better_contrast > 1] = 1\n","        #     print(lol.shape)\n","\n","        thresh = threshold_otsu(better_contrast)\n","        binary = better_contrast > thresh\n","        return binary  # assumes that you're using GPU\n","\n","    def get_binary(self, image_name):\n","        loader2 = transforms.Compose([transforms.Resize((256, 192)), transforms.ToTensor()])\n","        \"\"\"load image, returns cuda tensor\"\"\"\n","        image = Image.open(image_name)\n","        image = loader2(image).float()\n","        better_contrast = image.permute(1, 2, 0).detach().cpu().numpy()\n","        better_contrast[better_contrast > 1] = 1\n","        #     print(lol.shape)\n","\n","        thresh = threshold_otsu(better_contrast)\n","        binary = better_contrast > thresh\n","        return binary  # assumes that you're using GPU\n","\n","    def __getitem__(self, index):\n","        c_name = self.c_names[index]\n","        im_name = self.im_names[index]\n","\n","        # person image\n","        im = plt.imread(osp.join(self.data_path, 'image', im_name))\n","        cm = plt.imread(osp.join(self.data_path, 'cloth', c_name))\n","        wrap = plt.imread(osp.join(self.data_path, 'image_shaped_cloth', im_name))\n","        diff = plt.imread(osp.join(self.data_path, 'changed_diff', im_name))\n","        #         im = self.transform(im) # [-1,1]\n","\n","        # load parsing image\n","\n","        parse_name = im_name.replace('.jpg', '.png')\n","        im_parse = Image.open(osp.join(self.data_path, 'image-parse', parse_name))\n","        parse_array = np.array(im_parse)\n","        #         parse_shape = (parse_array > 0).astype(np.float32)\n","\n","        parse_head = (parse_array == 1).astype(np.float32) + \\\n","                     (parse_array == 2).astype(np.float32) + \\\n","                     (parse_array == 4).astype(np.float32) + \\\n","                     (parse_array == 13).astype(np.float32)\n","\n","        parse_cloth = (parse_array == 5).astype(np.float32) + (parse_array == 6).astype(np.float32) + (\n","                    parse_array == 7).astype(np.float32) + (parse_array == 9).astype(np.float32) + (\n","                                  parse_array == 15).astype(np.float32) + (parse_array == 3).astype(np.float32) + (\n","                                  parse_array == 14).astype(np.float32)\n","\n","        pcm = self.get_binary_from_img(parse_cloth)\n","        phead = self.get_binary_from_img(parse_head)  # [0,1]\n","        im_h = im * phead - (1 - phead)  # [-1,1], fill 0 for other parts\n","\n","        source = im * pcm\n","        source[source == 0] = 255\n","        mask = plt.imread(osp.join(self.data_path, 'nested_unet_msk', im_name))\n","\n","        lol = self.get_binary(osp.join(self.data_path, 'nested_unet_msk', im_name))\n","        lol2 = source * (1 - lol)\n","        lol2[lol2 == 0] = 255\n","\n","        lol3 = source * (lol)\n","        lol3[lol3 == 0] = 255\n","\n","        input = Image.fromarray(np.uint8(lol2))\n","        mask = Image.fromarray(np.uint8(mask))\n","        style = Image.fromarray(np.uint8(lol3))\n","        target = Image.fromarray(np.uint8(source))\n","        cloth = Image.fromarray(np.uint8(cm))\n","        wrap = Image.fromarray(np.uint8(wrap))\n","        diff = Image.fromarray(np.uint8(diff))\n","        head = Image.fromarray(np.uint8(im_h))\n","        #         source = self.transform_input(input)  # [-1,1]\n","        #         mask = self.transform_input(mask)  # [-1,1]\n","        style_ = self.transform(style)\n","        cloth = self.transform(cloth)\n","        head = self.transform(head)\n","        #         targ = self.transform_input(style)\n","        #         skel = self.transform_input(one_map)\n","\n","        resize = transforms.Resize(size=(128, 128))\n","        cloth = resize(cloth)  # Cloth ground truth, how it should look before applying\n","        style_ = resize(style_)\n","\n","        source, mask, target, targ, wrap, diff, head = self.transformData(input, mask, target, style, wrap, diff, head)\n","        del lol3, lol2, pcm, im, parse_cloth, im_parse, lol\n","        return source, mask, style_, target, targ, wrap, diff, cloth, head  # , skel\n","\n","    #\n","\n","    def __len__(self):\n","        return len(self.im_names)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOAVCMA-EaRx","executionInfo":{"status":"ok","timestamp":1634620913120,"user_tz":-330,"elapsed":1105,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","# ResNet module to process the incoming filters. We are using Instance Norm replacing traditional BatchNorm.\n","# BatchNorm doesn't plays any significant role, since our batch is very small, another thing we observed is\n","# that the feature maps don't face covariate shift in ResNet block as the dataset are very close to each other.\n","# removing Norm from ResNet block doesn't affects the model result.\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ResidualBlock, self).__init__()\n","\n","        conv_block = [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, in_features, 3),\n","                      nn.InstanceNorm2d(in_features),\n","                      nn.ReLU(inplace=True),\n","                      nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, out_features, 3),\n","                      nn.InstanceNorm2d(out_features)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ConvBlock, self).__init__()\n","\n","        conv_block = [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, in_features, 3),\n","                      nn.InstanceNorm2d(in_features),\n","                      nn.ReLU(inplace=True),\n","                      nn.ReflectionPad2d(1),\n","                      nn.Conv2d(in_features, out_features, 3),\n","                      nn.InstanceNorm2d(out_features)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","# Over the cause of GAN history we did infer that if we replace unknown region with Noise, then GANs can effectively\n","# generate the missing regions (effectively implies to generate something).\n","# We didn't test with different Noise, and their affects in detail.\n","class NoiseInjection(nn.Module):\n","    def __init__(self, channel):\n","        super().__init__()\n","\n","        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n","\n","    def forward(self, image, mask):\n","        #         pdb.set_trace()\n","        noise = torch.randn(1, 1, image.shape[2], image.shape[3])\n","        mask = mask[:, :1, :, :].repeat(1, image.shape[1], 1, 1)\n","        return image + self.weight * noise * mask\n","\n","\n","def swish(x):\n","    return x * F.sigmoid(x)\n","\n","\n","def get_mean_var(c):\n","    n_batch, n_ch, h, w = c.size()\n","\n","    c_view = c.view(n_batch, n_ch, h * w)\n","    c_mean = c_view.mean(2)\n","\n","    c_mean = c_mean.view(n_batch, n_ch, 1, 1).expand_as(c)\n","    c_var = c_view.var(2)\n","    c_var = c_var.view(n_batch, n_ch, 1, 1).expand_as(c)\n","    # c_var = c_var * (h * w - 1) / float(h * w)  # unbiased variance\n","\n","    return c_mean, c_var\n","\n","\n","# model_ds downsamples the feature maps, we use stride = 2 to downsample feature maps instead of\n","# max pooling layer which is not learnable.\n","class model_ds(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(model_ds, self).__init__()\n","\n","        conv_block = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n","                      nn.InstanceNorm2d(out_features),\n","                      nn.ReLU(inplace=True)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","# model_up Upsamples the feature maps again with a layer which is learnable, we didn't use any other method since\n","# nn.Upsample has no learnable weights, the other layer that we could have tried is sub-pixel which also learns to\n","# upsample / downsmaple.\n","class model_up(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(model_up, self).__init__()\n","\n","        conv_block = [nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n","                      nn.InstanceNorm2d(out_features),\n","                      nn.ReLU(inplace=True)]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","class transform_layer(nn.Module):\n","\n","    def __init__(self, input_nc, in_features, out_features):\n","        super(transform_layer, self).__init__()\n","        self.channels = in_features\n","\n","        self.convblock = ConvBlock(in_features + in_features, out_features)\n","        self.up_conv = nn.Conv2d(in_features * 2, in_features, 3, 1, 1)\n","        self.down_conv = nn.Sequential(\n","            nn.Conv2d(64, in_features // 4, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_features // 4, in_features // 2, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_features // 2, in_features, 1, 1),\n","            nn.ReLU()\n","        )\n","        self.noise = NoiseInjection(in_features)\n","\n","        self.convblock_ = ConvBlock(in_features + 64, out_features)\n","\n","        self.vgg_block = nn.Sequential(\n","            nn.Conv2d(input_nc, 16, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 1, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x, mask=None, style=None, mode='D'):\n","        #         pdb.set_trace()\n","        if mode == 'C':\n","            style = F.upsample(style, size=(x.shape[2], x.shape[2]), mode='bilinear')\n","\n","            style = self.vgg_block(style)\n","            concat = torch.cat([x, style], 1)\n","\n","            out = (self.convblock_(concat))\n","            return out, style\n","        else:\n","            mask = F.upsample(mask, size=(x.shape[2], x.shape[2]), mode='bilinear')\n","            x = self.noise(x, mask)\n","            #             style = F.upsample(style, size=(x.shape[2],x.shape[2]), mode='bilinear')\n","\n","            style = self.down_conv(style)\n","            concat = torch.cat([x, style], 1)\n","\n","            out = (self.convblock(concat) + style)\n","            return out\n","\n","\n","class transform_up_layer(nn.Module):\n","    def __init__(self, in_features, out_features, diff=False):\n","        super(transform_up_layer, self).__init__()\n","        self.channels = in_features\n","\n","        if diff == True:\n","            self.convblock = ConvBlock(in_features * 2 + in_features, out_features)\n","        else:\n","            self.convblock = ConvBlock(in_features * 2, out_features)\n","        self.up_conv = nn.Sequential(\n","            nn.Conv2d(in_features * 2, in_features, 3, 1, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x, y, mode=\"down\"):\n","\n","        y = self.up_conv(y)\n","        concat = torch.cat([x, y], 1)\n","\n","        out = self.convblock(concat)\n","\n","        #         out = self.adain(out,style)\n","\n","        return out\n","\n","\n","class GeneratorCoarse(nn.Module):\n","    def __init__(self, input_nc, output_nc, n_residual_blocks=1):\n","        super(GeneratorCoarse, self).__init__()\n","        in_features = 64\n","\n","        self.model_input_cloth = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_nc + 1, in_features, 7),\n","            nn.InstanceNorm2d(in_features),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.block128 = nn.Sequential(\n","            ResidualBlock(in_features, in_features)\n","        )\n","        self.block128_transform = transform_layer(input_nc, in_features, in_features)\n","\n","        self.block64 = nn.Sequential(\n","            model_ds(in_features, in_features * 2),\n","            ResidualBlock(in_features * 2, in_features * 2)\n","        )\n","        self.block64_transform = transform_layer(input_nc, in_features * 2, in_features * 2)\n","\n","        self.block32 = nn.Sequential(\n","            model_ds(in_features * 2, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block32_transform = transform_layer(input_nc, in_features * 4, in_features * 4)\n","\n","        self.block16 = nn.Sequential(\n","            model_ds(in_features * 4, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block16_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","        self.block8 = nn.Sequential(\n","            model_ds(in_features * 8, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block8_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","        self.block4 = nn.Sequential(\n","            model_ds(in_features * 8, in_features * 8),\n","            ResidualBlock(in_features * 8, in_features * 8)\n","        )\n","        self.block4_transform = transform_layer(input_nc, in_features * 8, in_features * 8)\n","\n","        self.block4_up = nn.Sequential(\n","            nn.Conv2d(in_features * 8, in_features * 4, 3, 1, 1),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block4_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block8_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block8_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block16_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block16_up_transform = transform_up_layer(in_features * 4, in_features * 8)\n","\n","        self.block32_up = nn.Sequential(\n","            model_up(in_features * 8, in_features * 4),\n","            ResidualBlock(in_features * 4, in_features * 4)\n","        )\n","        self.block32_up_transform = transform_up_layer(in_features * 2, in_features * 4, True)\n","\n","        self.block64_up = nn.Sequential(\n","            model_up(in_features * 4, in_features * 2),\n","            ResidualBlock(in_features * 2, in_features * 2)\n","        )\n","        self.block64_up_transform = transform_up_layer(in_features, in_features * 2, True)\n","\n","        self.block128_up = nn.Sequential(\n","            model_up(in_features * 2, in_features),\n","            ResidualBlock(in_features, in_features)\n","        )\n","\n","        self.block128_up_transform = transform_up_layer(in_features // 2, in_features, True)\n","\n","        self.model_output = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_features, output_nc, 7),\n","            nn.Tanh()\n","        )\n","    def _conv_layer_set(self, in_c, out_c):\n","        model_input = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_c, out_c, 7, padding=0),\n","            nn.InstanceNorm2d(out_c),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    # forward function\n","    def forward(self, src, *input):\n","        in_features = 64\n","        conds = []\n","        for cond in input:\n","            conds.append(cond)\n","        conds.append(src)\n","\n","        style = torch.cat(conds, 1)\n","        y = torch.cat([torch.randn(1, 1, src.shape[2], src.shape[3]), style], 1)\n","\n","        y = self.model_input_cloth(y)\n","\n","        y128 = self.block128(y)\n","        y128, s_128 = self.block128_transform(x=y128, style=style, mode=\"C\")\n","\n","        y64 = self.block64(y128)\n","        y64, s_64 = self.block64_transform(x=y64, style=style, mode=\"C\")\n","\n","        y32 = self.block32(y64)\n","        y32, s_32 = self.block32_transform(x=y32, style=style, mode=\"C\")\n","\n","        y16 = self.block16(y32)\n","        y16, s_16 = self.block16_transform(x=y16, style=style, mode=\"C\")\n","\n","        y8 = self.block8(y16)\n","        y8, s_8 = self.block8_transform(x=y8, style=style, mode=\"C\")\n","\n","        y4 = self.block4(y8)\n","        y4, s_4 = self.block4_transform(x=y4, style=style, mode=\"C\")\n","\n","        ############## Decoder #######################\n","\n","        y4u = self.block4_up(y4)\n","        y4u = self.block4_up_transform(y4u, y4)\n","\n","        y8u = self.block8_up(y4u)\n","        y8u = self.block8_up_transform(y8u, y8)\n","\n","        y16u = self.block16_up(y8u)\n","        y16u = self.block16_up_transform(y16u, y16)\n","\n","        y32u = self.block32_up(y16u)\n","\n","        y64u = self.block64_up(y32u)\n","\n","        y128u = self.block128_up(y64u)\n","\n","        out = self.model_output(y128u)\n","\n","        return out, s_128, s_64, s_32, s_16, s_8, s_4\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n","\n","        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n","        self.bn2 = nn.InstanceNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n","        self.bn3 = nn.InstanceNorm2d(128)\n","        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n","        self.bn4 = nn.InstanceNorm2d(128)\n","        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n","        self.bn5 = nn.InstanceNorm2d(256)\n","        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n","        self.bn6 = nn.InstanceNorm2d(256)\n","        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n","        self.bn7 = nn.InstanceNorm2d(512)\n","        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n","        self.bn8 = nn.InstanceNorm2d(512)\n","\n","        # Replaced original paper FC layers with FCN\n","        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        x = swish(self.conv1(x))\n","\n","        x = swish(self.bn2(self.conv2(x)))\n","        x = swish(self.bn3(self.conv3(x)))\n","        x = swish(self.bn4(self.conv4(x)))\n","        x = swish(self.bn5(self.conv5(x)))\n","        x = swish(self.bn6(self.conv6(x)))\n","        x = swish(self.bn7(self.conv7(x)))\n","        x = swish(self.bn8(self.conv8(x)))\n","\n","        x = self.conv9(x)\n","        return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLMRpP1WEf4t","executionInfo":{"status":"ok","timestamp":1634620913708,"user_tz":-330,"elapsed":590,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["def trainRefineModel(opt,netG, netD):\n","    print(\"training the model for different model types: %s\" % (opt.stage))\n","    dataset = refineDataSetExtract(128)\n","    train_loader = DataLoader(dataset,\n","                              batch_size=int(opt.batch),\n","                              shuffle=False,\n","                              num_workers=int(opt.thread),\n","                              drop_last=True, pin_memory=True)\n","\n","    epoch = 0\n","    \n","    \n","    n_epochs = int(opt.epochs)\n","    decay_epoch = int(opt.decay_epoch)\n","    batchSize = int(opt.batch)\n","    size = 128\n","    input_nc = int(opt.input_channel)\n","    output_nc = 3\n","    lr = float(opt.learn_rate)\n","    nRow = 4\n","\n","    criterion_GAN = torch.nn.MSELoss()\n","    criterion_identity = torch.nn.L1Loss()\n","\n","    optimizer_G = torch.optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n","    optimizer_D = torch.optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n","\n","    lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G,\n","                                                       lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","    lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D,\n","                                                       lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n","\n","    # Inputs & targets memory allocation\n","    Tensor = torch.FloatTensor\n","    input_A = Tensor(batchSize, input_nc, size, size)\n","\n","    target_real = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n","    target_fake = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n","\n","    fake_buffer = ReplayBuffer()\n","    print(n_epochs)\n","    for epoch in range(0, n_epochs):\n","        gc.collect()\n","        print(epoch)\n","        Source = iter(train_loader)\n","        avg_loss_g = 0\n","        avg_loss_d = 0\n","        for i in range(0, len(train_loader)):\n","            netG.train()\n","            target_real = Variable(torch.ones(1, 1), requires_grad=False)\n","            target_fake = Variable(torch.zeros(1, 1), requires_grad=False)\n","            optimizer_G.zero_grad()\n","\n","            #src, mask, style_img, target, gt_cloth, skel, cloth = Source.next()\n","            #src, mask, style_img, target, gt_cloth, skel, cloth = Variable(src), Variable(mask), Variable(style_img), Variable(target)\\\n","             #   , Variable(gt_cloth), Variable(skel), Variable(cloth)\n","\n","            src, mask, style_img, target, gt_cloth, wrap, diff, cloth, head = Source.next()\n","            src, mask, style_img, target, gt_cloth, wrap, diff, cloth, head = Variable(src), Variable(mask), Variable(style_img), Variable(target), Variable(gt_cloth), Variable(wrap), Variable(diff), Variable(cloth), Variable(head)\n","\n","\n","            #print(src.shape)\n","            #print(mask.shape)\n","            #print(style_img.shape)\n","            #print(target.shape)\n","            #print(gt_cloth.shape)\n","            #print(wrap.shape)\n","            #print(diff.shape)\n","            #print(cloth.shape)\n","\n","            gen_targ, _, _, _, _, _, _ = netG(diff, wrap)\n","            pred_fake = netD(gen_targ)\n","\n","            loss_GAN = 10 * criterion_GAN(pred_fake, target_real) + 10 * criterion_identity(gen_targ, target)\n","\n","            loss_G = loss_GAN\n","            loss_G.backward()\n","\n","            optimizer_G.step()\n","            #############################################\n","            optimizer_D.zero_grad()\n","\n","            pred_real = netD(target)\n","\n","            loss_D_real = criterion_GAN(pred_real, target_real)\n","\n","            # Fake loss\n","            gen_targ = fake_buffer.push_and_pop(gen_targ)\n","            pred_fake = netD(gen_targ.detach())\n","            loss_D_fake = criterion_GAN(pred_fake, target_fake)\n","\n","            # Total loss\n","            loss_D = (loss_D_real + loss_D_fake) * 0.5\n","            loss_D.backward()\n","\n","            if (i + 1) % int(opt.critic) == 0:\n","                optimizer_D.step()\n","\n","            avg_loss_g = (avg_loss_g + loss_G) / (i + 1)\n","            avg_loss_d = (avg_loss_d + loss_D) / (i + 1)\n","\n","            path_ = os.getcwd()\n","            root = path_ + '/gdrive/MyDrive/Datascience/ARProject/Tryon'\n","\n","            if (i + 1) % 50 == 0:\n","                print(\"Epoch: (%3d) (%5d/%5d) Loss: (%0.0003f) (%0.0003f)\" % (\n","                epoch, i + 1, len(train_loader), avg_loss_g * 1000, avg_loss_d * 1000))\n","\n","            #if (i + 1) % int(opt.display_count) == 0:\n","                #pic = (torch.cat([wrap,diff,gen_targ, target], dim=0).data + 1) / 2.0\n","                pic = (torch.cat([wrap, diff, gen_targ, target, head], dim=0).data + 1) / 2.0\n","\n","                pic1 = torch.cat([target], 0)  \n","                pic2 = torch.cat([head], 0)          \n","\n","                save_dir = \"{}/{}{}\".format(root, opt.results, opt.stage)\n","                if not os.path.exists(save_dir):\n","                  os.makedirs(save_dir)\n","                save_image(pic, '%s/Epoch_(%d)_(%dof%d).jpg' % (save_dir, epoch, i + 1, len(train_loader)), nrow=nRow)\n","                save_image(pic1, '%s/Epoch_Final_(%d)_(%dof%d).jpg' % (save_dir, epoch, i + 1, len(train_loader)))\n","                save_image(pic2, '%s/Epoch_Head_(%d)_(%dof%d).jpg' % (save_dir, epoch, i + 1, len(train_loader)))\n","\n","\n","            if (epoch + 1) % int(opt.save_model) == 0:\n","                save_dir = \"{}/{}{}\".format(root, opt.results, opt.stage)\n","                if not os.path.exists(save_dir):\n","                  os.makedirs(save_dir)\n","                torch.save(netG.state_dict(), '{}/Gan_{}.pth'.format(save_dir, epoch))\n","\n","            # Update learning rates\n","            lr_scheduler_G.step()\n","            lr_scheduler_D.step()\n","\n","            if i == 500 :\n","                break;\n","\n","        #if(epoch ==5):\n","         # break;\n","    print('traing refine complete')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6RL2J0lEo1L","executionInfo":{"status":"ok","timestamp":1634620913709,"user_tz":-330,"elapsed":12,"user":{"displayName":"Kiran Hemanthraj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09134364875487988858"}}},"source":["# define main function to start executing the training model\n","def main():\n","    trn_options = get_options()\n","    print(trn_options)\n","    print(\"Model training started\")\n","\n","    if not os.path.exists(trn_options.results):\n","        os.makedirs(trn_options.results)\n","    \n","    if trn_options.stage == \"Stitch\":\n","        netG = GeneratorCoarse(9, 3)\n","    else:\n","        netG = GeneratorCoarse(int(trn_options.input_channel), 3)\n","\n","    netD = Discriminator()\n","\n","    #intialize the weight for the model\n","    netG.apply(weights_init_normal)\n","    netD.apply(weights_init_normal)\n","\n","    if trn_options.stage == \"Shape\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainShapeModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    elif trn_options.stage == \"Stitch\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainStitchModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    elif trn_options.stage == \"Refine\":\n","        print(\"Training started for %s\" % (trn_options.stage))\n","        trainRefineModel(trn_options, netG, netD)\n","\n","        print(\"Training completed for %s \" % (trn_options.stage))\n","    else:\n","        print(\"Please mention the Stage from [Shape, Stitch, Refine]\")\n","\n","\n","    print('Finished training ')\n","\n","    sys.exit(\"Please mention the next Stage from [Shape, Stitch, Refine]\")\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIz6gOmGEwNH","outputId":"ecf096d6-752b-4a76-f6c3-bd6057e3b424"},"source":["if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["arguments are set for training the model\n","Namespace(batch='1', critic='10', data_list='train_pairs.txt', datamode='train', dataroot='Data', decay_epoch='10', display_count='500', epochs='21', input_channel='6', learn_rate='0.0002', results='results/', save_model='2', stage='Refine', thread='0')\n","Model training started\n","Training started for Refine\n","training the model for different model types: Refine\n","21\n","0\n"]}]}]}